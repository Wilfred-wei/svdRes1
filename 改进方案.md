**方案 B**（利用 ResCLIP 的 RCS 思想提取空间结构特征，并将其作为额外 Token 输入 FAFormer）在理论上非常契合伪造检测的任务特性。

**核心逻辑分析：**

* **ForgeLens 的 FAFormer** 旨在整合不同层级的特征，但它目前输入的只是各层的原始 CLS Token。这些 CLS Token 在深层往往已经“语义化”（关注“是一只猫”），丢失了“伪造痕迹”（关注“猫的眼睛不对劲”）。
* **ResCLIP 的 RCS** 证明了中间层（如 5-9 层）的注意力图（Attention Map）保留了极佳的空间定位能力。
* **结合点**：如果你能提取中间层的注意力模式，并用它去“重新聚合”最后一层的图像特征（Value），你就得到了一个**“带着中间层空间结构的深层特征 Token”**。将这个 Token 喂给 FAFormer，相当于给它开了一个“天眼”，直接告诉它哪里结构有问题。

以下是具体的实施步骤和代码修改方案：

### 步骤 1：改造 CLIP 的 Attention 机制

由于 ForgeLens 原生代码使用了 PyTorch 封装好的 `nn.MultiheadAttention`，这是一个黑盒，无法直接提取中间层的 Attention Map 或 Value。你需要用 ResCLIP 中的手动实现替换它。

**修改文件**：`models/network/clip/model.py`

首先，在 `VisionTransformer` 类中引入类似 ResCLIP 的 `custom_attn` 逻辑，并修改 `ResidualAttentionBlock`。

```python
# 在 models/network/clip/model.py 中添加/修改

class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()
        # 保持原有的初始化不变，但在 forward 中我们将不再直接调用 self.attn
        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = LayerNorm(d_model)
        self.attn_mask = attn_mask

    # 新增：手动计算 Attention 的函数 (提取自 ResCLIP)
    def manual_attn(self, x, return_map=False):
        # x: [L, B, D] -> [Seq_len, Batch, Dim]
        num_tokens, bsz, embed_dim = x.size()
        num_heads = self.attn.num_heads
        head_dim = embed_dim // num_heads
        scale = head_dim ** -0.5

        # 提取 Q, K, V
        q, k, v = F.linear(x, self.attn.in_proj_weight, self.attn.in_proj_bias).chunk(3, dim=-1)
        
        # 变换维度为 [B, Num_heads, Seq_len, Head_dim]
        q = q.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)

        # 计算 Attention Map [B*Num_heads, Seq_len, Seq_len]
        attn_weights = torch.bmm(q, k.transpose(1, 2)) * scale
        attn_weights = F.softmax(attn_weights, dim=-1)
        
        if return_map:
            return attn_weights, v
            
        # 标准输出计算
        attn_output = torch.bmm(attn_weights, v)
        attn_output = attn_output.transpose(0, 1).contiguous().view(num_tokens, bsz, embed_dim)
        attn_output = self.attn.out_proj(attn_output)
        return attn_output

    def forward(self, x, return_map=False):
        if return_map:
            # 仅用于提取 Attention Map，不改变原网络流
            return self.manual_attn(self.ln_1(x), return_map=True)
        else:
            # 保持原有的 Forward 逻辑 (使用标准attn或manual_attn均可，建议保持一致)
            # 这里为了不破坏预训练权重行为，主流还是走 self.attn，但为了提取 map 我们需要额外调用
            # 考虑到效率，建议在 Transformer 循环中统一处理
            x = x + self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x), need_weights=False, attn_mask=self.attn_mask)[0]
            x = x + self.mlp(self.ln_2(x))
            return x

```

### 步骤 2：在 Transformer 中计算 RCS Token

这是核心部分。我们需要在 `Transformer` 的 forward 循环中，缓存 5-9 层的 Attention Map，然后在最后一层计算出 RCS Token。

**修改文件**：`models/network/clip/model.py` 中的 `Transformer` 类

```python
class Transformer(nn.Module):
    # ... __init__ 保持不变 ...

    def forward(self, x: torch.Tensor):
        out = {}
        intermediate_attns = []
        rcs_token = None
        
        # 定义 ResCLIP 建议的层数范围 (例如 5-9 层)
        rcs_layers = range(5, 9) 
        
        for idx, layer in enumerate(self.resblocks.children()):
            # 1. 正常的前向传播 (ForgeLens 逻辑)
            x = layer(x)
            out[f'layer{idx}'] = x[0] # ForgeLens 保存每一层的输出
            
            # 2. Plan B 核心: 提取中间层 Attention
            if idx in rcs_layers:
                # 重新计算一次 attention map (为了不影响主流计算图，用 no_grad 也可以，但如果需要训练可能需要 grad)
                # 注意：ForgeLens 是冻结 Backbone 的，所以这里不需要梯度，可以用 torch.no_grad()
                with torch.no_grad():
                     # 我们需要 layer.ln_1(input_x) 作为输入，但这里 x 已经是输出了。
                     # 为了严谨，你应该在 layer(x) 之前保存 input。
                     # 简化写法：我们修改上面的 ResidualAttentionBlock.forward 返回 map
                     pass 

        # --- 更优雅的写法是重写上面的循环 ---
        x_input = x
        for idx, layer in enumerate(self.resblocks.children()):
            # 提取 Attention Map (如果是目标层)
            if idx in rcs_layers:
                # 注意：这里需要传入归一化后的输入
                attn_map, _ = layer.manual_attn(layer.ln_1(x_input), return_map=True)
                intermediate_attns.append(attn_map)
            
            # 如果是最后一层，我们需要它的 Value (V) 来生成 Token
            if idx == self.layers - 1:
                _, v_last = layer.manual_attn(layer.ln_1(x_input), return_map=True)
            
            # 正常执行层计算
            x_input = layer(x_input)
            out[f'layer{idx}'] = x_input

        # 3. 计算 RCS Token
        # 聚合中间层 Attention (Shape: [Batch*Heads, Seq_len, Seq_len])
        if len(intermediate_attns) > 0:
            avg_attn = torch.stack(intermediate_attns).mean(dim=0)
            
            # 我们只关心 CLS token 对其他 patch 的注意力 (Row 0)
            # avg_attn shape: [B*H, N+1, N+1], 取 [:, 0:1, :] -> CLS row
            rcs_attn_cls = avg_attn[:, 0:1, :] 
            
            # 利用 RCS Attention 聚合最后一层的 Value
            # v_last shape: [B*H, N+1, Head_Dim]
            # rcs_head_out = rcs_attn_cls @ v_last -> [B*H, 1, Head_Dim]
            rcs_head_out = torch.bmm(rcs_attn_cls, v_last)
            
            # 还原维度 [B, H, 1, D_h] -> [B, 1, D]
            bsz = x.shape[1] # 注意 x 是 [Seq, Batch, Dim]
            embed_dim = x.shape[2]
            rcs_token = rcs_head_out.view(bsz, -1, embed_dim).transpose(0, 1) # [1, B, D]
            
            # 通过最后一层的 Out Project (可选，为了匹配特征空间)
            last_block = self.resblocks[-1]
            rcs_token = last_block.attn.out_proj(rcs_token)

        return out, x_input, rcs_token

```

### 步骤 3：将 RCS Token 传递给 FAFormer

修改 `net_stage1` 和 `VisionTransformer` 以输出这个新 Token，并在 Stage 2 训练时使用。

**修改文件**：`models/network/net_stage1.py`

```python
class net_stage1(nn.Module):
    # ...
    def forward(self, x):
        # 此时 backbone.encode_image 需要被修改为返回 (features, cls_tokens, rcs_token)
        feature, cls_tokens, rcs_token = self.backbone.encode_image(x)
        
        result = self.fc(feature)
        
        # 将 RCS Token 加入返回列表
        return result, cls_tokens, rcs_token

```

### 步骤 4：FAFormer 的输入融合

FAFormer 接收的是一个 Token 序列。你只需要简单地将 `rcs_token` 拼接到 `cls_tokens` 列表中即可。

假设在 `trainer_stage2.py` 或 `net_faformer.py` 中：

```python
# 伪代码：在构建 FAFormer 输入时
# original_tokens = [cls_layer1, cls_layer2, ..., cls_layer12]
# rcs_token = [rcs_token_calculated]

# 拼接
input_tokens = torch.cat(cls_tokens + [rcs_token], dim=0) 

# 输入 FAFormer
# FAFormer 会自动处理序列长度变化（因为它是 Transformer 结构）
out = self.faformer(input_tokens)

```

### 方案总结与优势

1. **数据高效（Data-Efficient）**：你没有引入任何新的可训练参数来生成这个 `rcs_token`（它是完全基于冻结权重的统计量计算出来的），这完美符合 ForgeLens "Data-Efficient" 的核心主张。
2. **即插即用**：不需要重新预训练 CLIP，只是改变了前向传播的计算流。
3. **互补性强**：
* **WSGM** 训练每一层去关注伪造。
* **RCS Token** 直接利用了未被微调但空间感知极强的中间层先验。
* 两者结合，相当于既有“后天学习”（WSGM），又有“先天直觉”（RCS）。
